{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(datapath):\n",
    "    \"\"\"read data with datetime index\n",
    "    Args:\n",
    "       datapath(str):csv file\n",
    "    Returns:\n",
    "       rawdata(dataframe)\n",
    "    \"\"\"\n",
    "    rawdata=pd.read_csv(datapath)\n",
    "    rawdata['time'] = pd.to_datetime(rawdata['datetime'],format=\"%Y/%m/%d %H:%M\")\n",
    "    rawdata['time']=rawdata['time'].apply(pd.Timestamp)\n",
    "    rawdata.index=rawdata['time']\n",
    "    rawdata=rawdata.drop(['datetime','time'], axis=1)\n",
    "    return rawdata\n",
    "\n",
    "def data_plot(rawdata):\n",
    "    \"\"\" 1. plot the data in time series and mark the downtimes\n",
    "    Args:\n",
    "       rawdata(dataframe):   \n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    column_name=rawdata.columns\n",
    "    column_num=len(column_name)\n",
    "    plt.figure(figsize=(50,column_num*10)).patch.set_facecolor('white')\n",
    "    data_P=rawdata\n",
    "    \n",
    "    for idx, i in enumerate(column_name):\n",
    "        plt.subplot(column_num,1,idx+1)\n",
    "        plt.title(i,fontsize=12)\n",
    "        plt.scatter(rawdata.index,rawdata[i])\n",
    "        plt.title(i,fontsize=70,alpha=0.5)\n",
    "        plt.ylabel(i,fontsize=50)\n",
    "        plt.yticks(fontsize=40)\n",
    "        plt.xticks(fontsize=40,rotation=45)\n",
    "        plt.legend(fontsize=40,loc='upper left')\n",
    "        \n",
    "        #mark the downtime\n",
    "        plt.axvspan(\"2017-03-18 13:05:00\",\"2017-03-18 18:05:00\",alpha=0.3,color=\"g\")\n",
    "        plt.axvspan(\"2017-10-27 08:55:00\",\"2017-10-28 00:10:00\",alpha=0.3,color=\"g\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"pca_analysis.png\")\n",
    "\n",
    "def kmeans_PCA(data,PCA_n=2,Kmeans_n):\n",
    "    \"\"\" 1. The silhouette plot for the various clusters on 2D PCA\n",
    "        2. The visualization of the clustered data on 2D PCA\n",
    "        3. plot the data with PCA label in time series to mark abnormal conditions and mark the downtimes\n",
    "        4. plot the different PC in time series and mark the downtimes\n",
    "    Args:\n",
    "       rawdata(dataframe)\n",
    "       PCA_n(int): PCA dimensions(2)\n",
    "       Kmeans_n(int):the numbers of cluster\n",
    "       \n",
    "    Returns:\n",
    "       cluster_labels(numpy):cluster_labels in time series to mark diffent equipment conditions\n",
    "    \"\"\"\n",
    "    from sklearn import preprocessing\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    import matplotlib.cm as cm\n",
    "    from sklearn.datasets import make_blobs\n",
    "    from sklearn.cluster import KMeans\n",
    "    from sklearn import datasets\n",
    "    from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "    from sklearn.decomposition import PCA \n",
    "    %matplotlib inline\n",
    "    plt.style.use('ggplot')\n",
    "    \n",
    "    x = data\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled)\n",
    "\n",
    "    pca=PCA(n_components=PCA_n)\n",
    "    X=pca.fit_transform(df)\n",
    "    range_n_clusters = [Kmeans_n]\n",
    "    \n",
    "    # 計算並繪製輪廓分析的結果\n",
    "    for n_clusters in range_n_clusters:\n",
    "        # Create a subplot with 1 row and 2 columns\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "        fig.set_size_inches(18, 7)\n",
    "\n",
    "        # The 1st subplot is the silhouette plot\n",
    "        # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "        # lie within [-0.1, 1]\n",
    "        ax1.set_xlim([-0.1, 1])\n",
    "        # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "        # plots of individual clusters, to demarcate them clearly.\n",
    "        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "        # Initialize the clusterer with n_clusters value and a random generator\n",
    "        # seed of 10 for reproducibility.\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "        cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "        # The silhouette_score gives the average value for all the samples.\n",
    "        # This gives a perspective into the density and separation of the formed\n",
    "        # clusters\n",
    "        silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "        print(\"For n_clusters =\", n_clusters,\n",
    "              \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "        # Compute the silhouette scores for each sample\n",
    "        sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "        y_lower = 10\n",
    "        for i in range(n_clusters):\n",
    "            # Aggregate the silhouette scores for samples belonging to\n",
    "            # cluster i, and sort them\n",
    "            ith_cluster_silhouette_values = \\\n",
    "                sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0, ith_cluster_silhouette_values,\n",
    "                              facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "            # Label the silhouette plots with their cluster numbers at the middle\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            # Compute the new y_lower for next plot\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        # The vertical line for average silhouette score of all the values\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        # 2nd Plot showing the actual clusters formed\n",
    "        colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "        ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7,\n",
    "                    c=colors, edgecolor='k')\n",
    "\n",
    "        # Labeling the clusters\n",
    "        centers = clusterer.cluster_centers_\n",
    "        # Draw white circles at cluster centers\n",
    "        ax2.scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                    c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "        for i, c in enumerate(centers):\n",
    "            ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                        s=50, edgecolor='k')\n",
    "\n",
    "        ax2.set_title(\"The visualization of the clustered data.\")\n",
    "        ax2.set_xlabel(\"Feature space for the 1st feature\")\n",
    "        ax2.set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "        plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "                      \"with n_clusters = %d\" % n_clusters),\n",
    "                     fontsize=14, fontweight='bold')\n",
    "        plt.savefig(\"pca_clusters.png\")\n",
    "        \n",
    "    label=pd.DataFrame(cluster_labels,columns=['Equipment_Conditon'],index=data.index)\n",
    "    x=pd.DataFrame(X,index=data.index)\n",
    "    data=pd.concat([x,label],axis=1)\n",
    "    data_plot(label)\n",
    "    plt.savefig(\"Equipment_condition.png\")\n",
    "    data_plot(x)\n",
    "    plt.savefig(\"pca_comps.png\")\n",
    "    plt.show()\n",
    "    return cluster_labels\n",
    "\n",
    "def PCA_FeatruenImportantancePlot(dataset,n_comps):\n",
    "    \"\"\" plot the feature importantance in PCA analysis\n",
    "    Args:\n",
    "       dataset(dataframe)\n",
    "       n_comps(int): PCA dimensions\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn import preprocessing\n",
    "    x = dataset\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled)\n",
    "    pca = PCA(n_components=n_comps, svd_solver='full')\n",
    "    pca.fit(df)\n",
    "    print(\"Variance Ratio --  {} principle components\".format(n_comps)) \n",
    "    print(pca.explained_variance_ratio_)\n",
    "    pca_ComDistr = pd.DataFrame(pca.components_, columns = dataset.columns)\n",
    "    Feature_Importance=pd.DataFrame()\n",
    "    for i in pca_ComDistr:\n",
    "        importance=[]\n",
    "        for j in range(pca_ComDistr.shape[0]):\n",
    "            a=abs(pca_ComDistr.loc[j,i])*pca.explained_variance_ratio_[j]\n",
    "            importance.append(a)\n",
    "        Feature_Importance.insert(0,i,[sum(importance)])\n",
    "    Feature_Importance=Feature_Importance.sort_values(by=0, ascending=False, axis=1)\n",
    "    x=Feature_Importance.columns\n",
    "    x_num=np.arange(len(x))\n",
    "    y=Feature_Importance.loc[0,:]\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.barh(x_num,y)\n",
    "    plt.yticks(x_num, x)\n",
    "    plt.title(\"PCA({} components)_Feature Importance\".format(n_comps),fontsize=20,alpha=0.5)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylabel(\"Feature\",fontsize=10)\n",
    "    plt.xlabel(\"Importance\",fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"PCA({} components)_Feature Importance.png\".format(n_comps))\n",
    "    plt.show()\n",
    "    \n",
    "def PcaDistrutionPlot(dataset,PcaNum,pca_n):\n",
    "    \"\"\" plot the feature Distrutions in diffenent PCs\n",
    "    Args:\n",
    "       dataset(dataframe)\n",
    "       PcaNum(int): PCA dimensions\n",
    "       pca_n(int): PC number\n",
    "    Returns:\n",
    "       None\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    from sklearn import preprocessing\n",
    "    \n",
    "    x = dataset\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    x_scaled = min_max_scaler.fit_transform(x)\n",
    "    df = pd.DataFrame(x_scaled)\n",
    "    n_comps=PcaNum\n",
    "    pca = PCA(n_components=n_comps, svd_solver='full')\n",
    "    pca.fit(df)\n",
    "\n",
    "    pca_ComDistr = pd.DataFrame(pca.components_, columns = dataset.columns)\n",
    "    c={}\n",
    "    for i in pca_ComDistr:\n",
    "        if pca_ComDistr.loc[PcaNum-1,i]>0:\n",
    "            c[i]=\"salmon\"\n",
    "        else:\n",
    "            c[i]=\"teal\"\n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    abs_pca_ComDistr=abs(pca_ComDistr.loc[pca_n,:])\n",
    "    pca_ComDistr_sort=pd.DataFrame(abs_pca_ComDistr).T.sort_values(by=pca_n, ascending=False, axis=1)\n",
    "    x=pca_ComDistr_sort.columns\n",
    "    x_num=np.arange(len(x))\n",
    "    y=pca_ComDistr_sort.loc[pca_n,:]\n",
    "    plt.barh(x_num,\n",
    "            y,color=[c[r] for r in x])\n",
    "    plt.yticks(x_num, x)\n",
    "    plt.title(\"PCA{}-{}_Feature Distribution\".format(PcaNum,pca_n+1),fontsize=20,alpha=0.5)\n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.ylabel(\"Feature\",fontsize=10)\n",
    "    plt.xlabel(\"Distribution\",fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"PCA{}-{}_Feature Distribution.png\".format(PcaNum,pca_n+1))\n",
    "    plt.show()\n",
    "    \n",
    "def PCA_analysis(data,PcaNum,Kmeans_n):\n",
    "    \"\"\" 1. PCA&Kmeans analysis report to know the equipment conditons\n",
    "        2. feature importance in PCA analysis\n",
    "        3. feature distribution in diffenet PCs\n",
    "    Args:\n",
    "       data(dataframe)\n",
    "       PcaNum(int): the numbers of PCA dimensions\n",
    "       Kmeans_n(int): the numbers of cluster\n",
    "    Returns:\n",
    "       cluster_labels(numpy):cluster_labels in time series to mark diffent equipment conditions\n",
    "    \"\"\"\n",
    "    cluster_labels=kmeans_PCA(data,PcaNum,Keams_n)\n",
    "    PCA_FeatruenImportantancePlot(data,PcaNum)\n",
    "    for i in range(PcaNum):\n",
    "        PcaDistrutionPlot(data,PcaNum,i)\n",
    "    return cluster_labels"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf-gpu)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
